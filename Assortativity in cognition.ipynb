{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Read Me #\n",
    "###########\n",
    "\n",
    "# Compile the first cell below to import libraries.\n",
    "# The second cell below is the code of the simulation.\n",
    "# The other cells, except for the last and the second last, are the codes of single steps of the simulation.\n",
    "# Use the code from the second last cell to perform and save simulations for different combinations of parameters.\n",
    "# Use the code from the last cell to obtain a graphical representetion in the k-p plane. \n",
    "\n",
    "# Parameters:\n",
    "    # M: number of couples;\n",
    "    # N: number of agents, N=M*2;\n",
    "    # k: probability of intuitive response;\n",
    "    # p: probability of one-shot prisoner's dilemma;\n",
    "    # A: probability of assortativity in cognition for each couple;\n",
    "    # T: Number of time steps in a simulation round;\n",
    "    # vector_agents: vector of integer numbers, ranging from 0 to N-1, N=M*2. Each agent is represented by a number;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Import libraries #\n",
    "####################\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import sympy as sym \n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Simulation #\n",
    "##############\n",
    "\n",
    "def simulation(T,p,k,A,M):\n",
    "    \n",
    "    N=M*2\n",
    "    \n",
    "    vector_agents= [i for i in range(N)]\n",
    "    \n",
    "    memory = np.ones([N,2])                     # Initial memory of agents: both memories of cooperation and defection are equal to one;\n",
    "    \n",
    "    total_cooperation_s1=[]                     # Generate empty set. In each period the new value is added. At the end of the simulation a complete time series is generated;         \n",
    "    total_cooperation_tot=[]                                     \n",
    "    total_reward_t=[]  \n",
    "    reward_s1_t=[]     \n",
    "    \n",
    "    for t in range(T):                                                                  # In each period:\n",
    "            \n",
    "        Matching=matching(M,vector_agents)                                                  # Match agents in random couples\n",
    "            \n",
    "        Random_game=random_game(p,M)                                                        # Select the game of each couple\n",
    "            \n",
    "        Choice_s1=choice_s1(M,memory)                                                       # Select the intuitive response of each agent\n",
    "            \n",
    "        Choice_s2=choice_s2(M,Random_game,Matching)                                         # Select the deliberative response of each agent   \n",
    "        \n",
    "        System=system_choice(M,A,k,Matching)                                                # Select the cognitive system of each agent\n",
    "            \n",
    "        Choice=choice(M,System,Choice_s1,Choice_s2)                                         # Select the strategy of each agent, given the choice of cognitive system    \n",
    "            \n",
    "        Reward=reward(M,Random_game,Choice,Matching)                                        # Calculate the reward of each agents    \n",
    "        \n",
    "        Cooperation_s1=cooperation(M,Choice,memory)                                         # Calculate cooperation under intuition    \n",
    "        \n",
    "        Cooperation_tot=cooperation_tot(M,Choice)                                           # Calculate total cooperation   \n",
    "\n",
    "        total_cooperation_s1.append(Cooperation_s1/(N))                                     # Calculate cooperation rate under intuition      \n",
    "            \n",
    "        total_cooperation_tot.append(Cooperation_tot/(N))                                   # Calculate total cooperation rate        \n",
    "            \n",
    "        total_reward=sum(Reward)/(M)                                                        # Calculate average reward            \n",
    "        \n",
    "        total_reward_t.append((total_reward[0]+total_reward[1])/2)                          # Generate a time serie of average total reward\n",
    "\n",
    "        Reward_s1=reward_intuition(M,System,Reward,Matching)                                # Calculate average reward under intuition\n",
    "        \n",
    "        reward_s1_t.append(Reward_s1)                                                       # Generate a time serie of average reward under intuition\n",
    "        \n",
    "        ##################################################### \n",
    "        # Select one of the two following update strategies #\n",
    "        #####################################################################################\n",
    "        memory=update_reinforce(M,Choice,memory,Reward,vector_agents)                       # Agents update memories: reinforcement learning\n",
    "        #memory=update_memory_1(M,Choice,memory,Reward,vector_agents)                        # Agents update memories: last reward    \n",
    "        #####################################################################################\n",
    "        \n",
    "    total_reward_T=sum(total_reward_t)/T                                                # Calculate the average reward along the entire simulation\n",
    "\n",
    "    reward_s1_T=sum(reward_s1_t)/T                                                      # Calculate the average reward under intuition along the entire simulation\n",
    "\n",
    "    mean_coop_s1=sum(total_cooperation_s1)/T                                            # Calculate the average cooperation rate under intuition along the entire simulation \n",
    "\n",
    "    mean_coop_tot=sum(total_cooperation_tot)/T                                          # Calculate the average total cooperation rate along the entire simulation   \n",
    "\n",
    "    return(mean_coop_s1, mean_coop_tot)                                                 # Return the average cooperation rate under intuition and the average total cooperation rate\n",
    "                                                                                        # Other results can be added, such as the mean average reward under intuition;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Random Matching #\n",
    "###################\n",
    "\n",
    "def matching(M,vector_agents):\n",
    "    N=M*2\n",
    "    couples=np.ones([M,2])\n",
    "    rd.seed()\n",
    "    rd.shuffle(vector_agents)                                           # Randomize the order of agents\n",
    "    for i in np.arange(0,M,1):\n",
    "        couples[i]=[int(vector_agents[i]),int(vector_agents[N-1-i])]    # Match the i-first and the i-last\n",
    "    return(couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Choice of the game played #\n",
    "#############################\n",
    "\n",
    "def random_game(p,M):         # game=0: one-shot prisoner's dilemma; game=1 repeated prisoner's dilemma\n",
    "    N=M*2\n",
    "    game=np.zeros(M)\n",
    "    for i in range(M):              # Loop inside couples\n",
    "        if p<=rd.uniform(0, 1):         # With probability 1-p\n",
    "            game[i]=0                       # Couple i plays the one-shot prisoner's dilemma\n",
    "        else:                           # With probability p\n",
    "            game[i]=1                       # Couple i plays the repeated prisoner's dilemma\n",
    "    return(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Payoff matrix of one-shot prisoner's dilemma # \n",
    "################################################\n",
    "\n",
    "#     C    D\n",
    "#  C 4,4  0,5\n",
    "#  D 5,0  1,1\n",
    "\n",
    "# action0: action of row player; 0 for cooperation, C; 1 for defection, D;\n",
    "# action1: action of column player; 0 for cooperation, C; 1 for defection, D;\n",
    "\n",
    "payoff_matrix_Game_0=np.zeros(2)\n",
    "def Game_0(action0,action1):\n",
    "    if action0==0 and action1==0:          # Both the agents cooperate;\n",
    "        payoff_matrix_Game_0=[4,4]\n",
    "    elif action0==0 and action1==1:        # Row player cooperates, column player defects;\n",
    "        payoff_matrix_Game_0=[0,5]\n",
    "    elif action0==1 and action1==0:        # Row player defects, column player cooperates;\n",
    "        payoff_matrix_Game_0=[5,0]\n",
    "    else:                                  # Both the agents defect;\n",
    "        payoff_matrix_Game_0=[1,1]\n",
    "    return(payoff_matrix_Game_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Payoff matrix of repeated prisoner's dilemma #\n",
    "################################################\n",
    "\n",
    "#     C    D\n",
    "#  C 4,4  1,1\n",
    "#  D 1,1  1,1\n",
    "\n",
    "# action0: action of row player; 0 for cooperation, C; 1 for defection, D;\n",
    "# action1: action of column player; 0 for cooperation, C; 1 for defection, D;\n",
    "\n",
    "payoff_matrix_Game_1=np.zeros(2)\n",
    "def Game_1(action0,action1):\n",
    "    if action0==0 and action1==0:          # Both the agents cooperate;\n",
    "        payoff_matrix_Game_1=[4,4]\n",
    "    elif action0==0 and action1==1:        # Row player cooperates, column player defects;\n",
    "        payoff_matrix_Game_1=[1,1]\n",
    "    elif action0==1 and action1==0:        # Row player defects, column player cooperates;\n",
    "        payoff_matrix_Game_1=[1,1]\n",
    "    else:                                  # Both the agents defect;\n",
    "        payoff_matrix_Game_1=[1,1]\n",
    "    return(payoff_matrix_Game_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Choice of Agents under System 1 #\n",
    "###################################\n",
    "\n",
    "# memory: memory of agents:\n",
    "    # memory[i][0]: memory of agent i relative to action C;\n",
    "    # memory[i][1]: memory of agent i relative to action D;\n",
    "\n",
    "def choice_s1(M,memory):                         # actions_s1=0: intuitive cooperation; actions_s1=1: intuitive defection\n",
    "    N=M*2\n",
    "    actions_s1= np.zeros(N)\n",
    "    for i in range(N):                           # Loop inside agents\n",
    "        rd.seed()\n",
    "        if memory[i][0]>memory[i][1]:                # Memory of cooperation is greater than memory of defection\n",
    "            actions_s1[i]=0                              # Intuitive cooperation\n",
    "        elif memory[i][0]<memory[i][1]:              # Memory of defection is greater than memory of cooperation\n",
    "            actions_s1[i]=1                              # Intuitive defection\n",
    "        elif memory[i][0]==memory[i][1]:             # Memories of cooperation and defection are equal\n",
    "            actions_s1[i]=rd.randint(0, 1)               # Random intuitive response; 50% each strategy\n",
    "    return(actions_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Choice of Agents under System 2 #\n",
    "###################################\n",
    "\n",
    "def choice_s2(M,Random_game,Matching):\n",
    "    N=M*2\n",
    "    actions_s2= np.zeros(N)\n",
    "    for i in range(M):                                  # loop inside couples\n",
    "        if Random_game[i]==0:                               # Couple i plays Game_0   \n",
    "            actions_s2[int(Matching[i][0])]=1                   # The first agent of the couple defects under deliberation\n",
    "            actions_s2[int(Matching[i][1])]=1                   # The second agent of the couple defects under deliberation\n",
    "        else:                                               # Couple i plays Game_1\n",
    "            actions_s2[int(Matching[i][0])]=0                   # The first agent of the couple cooperates under deliberation\n",
    "            actions_s2[int(Matching[i][1])]=0                   # The second agent of the couple cooperates under deliberation\n",
    "    return(actions_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Choice of Cognitive System #\n",
    "##############################\n",
    "\n",
    "def system_choice(M,A,k,Matching):      # system=0: Intuition; system=1: Deliberation\n",
    "    N=M*2\n",
    "    system=np.zeros(N)            \n",
    "    for i in range(M):                              # Loop inside couples\n",
    "        if A>=rd.uniform(0, 1):                         # With probability A\n",
    "            if k>=rd.uniform(0, 1):                         # With probability k\n",
    "                system[int(Matching[i][0])]=0                   # First agent of the couple is intuitive\n",
    "                system[int(Matching[i][1])]=0                   # Second agent of the couple is intuitive\n",
    "            else:                                           # With probability 1-k\n",
    "                system[int(Matching[i][0])]=1                   # First agent of the couple is deliberative\n",
    "                system[int(Matching[i][1])]=1                   # Second agent of the couple is deliberative\n",
    "        else:                                           # With probability 1-assortativity\n",
    "            if k>=rd.uniform(0, 1):                         # With probability k\n",
    "                system[int(Matching[i][0])]=0                   # First agent of the couple is intuitive\n",
    "            else:                                           # With probability 1-k\n",
    "                system[int(Matching[i][0])]=1                   # First agent of the couple is deliberative\n",
    "            if k>=rd.uniform(0, 1):                         # With probability k\n",
    "                system[int(Matching[i][1])]=0                   # Second agent of the couple is intuitive\n",
    "            else:                                           # With probability 1-k\n",
    "                system[int(Matching[i][1])]=1                   # Second agent of the couple is deliberative                    \n",
    "    return(system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Choice of Agents #\n",
    "####################\n",
    "\n",
    "def choice(M,System,Choice_s1,Choice_s2):\n",
    "    N=M*2\n",
    "    actions=np.zeros(N)\n",
    "    for i in range(N):                                # Loop inside agents          \n",
    "        if System[i]==0:                                  # The agent is intuitive            \n",
    "            actions[i]=Choice_s1[i]                           # Choice of the agent under System 1               \n",
    "        else:                                             # The agent is deliberative\n",
    "            actions[i]=Choice_s2[i]                           # Choice of the agent under System 2               \n",
    "    return(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Reward of Agents #\n",
    "####################\n",
    "\n",
    "def reward(M,Random_game,Choice,Matching):\n",
    "    result=np.zeros([M,2])\n",
    "    for i in range(M):                                                                   #Loop inside couples\n",
    "        if Random_game[i]==0:                                                                # Game_0 occurs\n",
    "            result[i]=Game_0(Choice[int(Matching[i][0])],Choice[int(Matching[i][1])])            # Payoffs given the choices of the two agents forming the couple\n",
    "        else:                                                                                # Game_1 occurs\n",
    "            result[i]=Game_1(Choice[int(Matching[i][0])],Choice[int(Matching[i][1])])            # Payoffs given the choices of the two agents forming the couple           \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Reward of Agents under intuition #\n",
    "####################################\n",
    "\n",
    "def reward_intuition(M,System,Reward,Matching):\n",
    "    N=M*2\n",
    "    reward_s1=0\n",
    "    for i in range(M):                                         # Loop inside couples\n",
    "        if System[int(Matching[i][0])]==0:                         # The first agent in the couple responds intuitively\n",
    "            reward_s1=reward_s1+Reward[i][0]                           # Sum the reward\n",
    "        if System[int(Matching[i][1])]==0:                         # The second agent in the couple responds intuitively\n",
    "            reward_s1=reward_s1+Reward[i][1]                           # Sum the reward\n",
    "    if sum(System)==N:                                         # All the agents deliberate\n",
    "        mean_reward_s1=0                                           # Average reward under intuition equal to zero\n",
    "    elif sum(System) < N:                                      # There is intuitive response\n",
    "        mean_reward_s1=reward_s1/(N-sum(System))                   # Calculate the average reward\n",
    "    return(mean_reward_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Number of intuitive cooperators #\n",
    "###################################\n",
    "\n",
    "# Notice that agents with memories of cooperation and defection equal are counted as one half;\n",
    "\n",
    "def cooperation(M,Choice,memory):\n",
    "    N=M*2\n",
    "    coop_s1=0\n",
    "    for i in range(N):                      # Loop inside agents\n",
    "        if memory[i][0]>memory[i][1]:           # Agent with memory of cooperation greater than memory of defection\n",
    "            coop_s1=coop_s1+1                       # Sum one\n",
    "        elif memory[i][0]==memory[i][1]:        # Agent with memory of cooperation equal to memery of defection\n",
    "            coop_s1=coop_s1+0.5                     # Sum one half\n",
    "    return(coop_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Number of effective cooperators #\n",
    "###################################\n",
    "\n",
    "def cooperation_tot(M,Choice):\n",
    "    N=M*2\n",
    "    coop_tot=0                   \n",
    "    for i in range(N):                   # Loop inside agents                    \n",
    "        if Choice[i]==0:                     # The agent cooperates\n",
    "            coop_tot=coop_tot+1                  # Sum one           \n",
    "    return(coop_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Update Memory # Memory length equal to one #\n",
    "##############################################\n",
    "\n",
    "def update_memory_1(M,Choice,memory,Reward,vector_agents):\n",
    "    N=M*2\n",
    "    for i in range(M):                                                                 # Loop inside first M agents           \n",
    "        if Choice[(int(vector_agents[i]))]==0:                                             # The agent cooperates\n",
    "            memory[(int(vector_agents[i]))][0]=Reward[i][0]                                    # Update memory of cooperation\n",
    "            memory[(int(vector_agents[i]))][1]=memory[(int(vector_agents[i]))][1]              # Memory of defection unchanged\n",
    "                        \n",
    "        else:                                                                              # The agent defects\n",
    "            memory[(int(vector_agents[i]))][0]=memory[(int(vector_agents[i]))][0]              # Memory of cooperation unchanged\n",
    "            memory[(int(vector_agents[i]))][1]=Reward[i][0]                                    # Update memory of defection\n",
    "                        \n",
    "    for i in range(M,N):                                                               # Loop inside second M agents\n",
    "        if Choice[(int(vector_agents[i]))]==0:                                             # The agent cooperates\n",
    "            memory[(int(vector_agents[i]))][0]=Reward[N-1-i][1]                                # Update memory of cooperation\n",
    "            memory[(int(vector_agents[i]))][1]=memory[(int(vector_agents[i]))][1]              # Memory of defection unchanged\n",
    "                        \n",
    "        else:                                                                              # The agent defects\n",
    "            memory[(int(vector_agents[i]))][0]=memory[(int(vector_agents[i]))][0]              # Memory of cooperation unchanged\n",
    "            memory[(int(vector_agents[i]))][1]=Reward[N-1-i][1]                                # Update memory of defection\n",
    "    return(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Update Memory # Reinforcement learning #\n",
    "##########################################\n",
    "\n",
    "def update_reinforce(M,Choice,memory,Reward,vector_agents):\n",
    "    N=M*2\n",
    "    for i in range(M):                                                                                   # Loop inside first M agents\n",
    "        if Choice[(int(vector_agents[i]))]==0:                                                               # The agent cooperates\n",
    "            memory[(int(vector_agents[i]))][0]=(memory[(int(vector_agents[i]))][0]+Reward[i][0])/2               # Update memory of cooperation\n",
    "            memory[(int(vector_agents[i]))][1]=memory[(int(vector_agents[i]))][1]                                # Memory of defection unchanged\n",
    "                       \n",
    "        else:                                                                                                # The agent defects\n",
    "            memory[(int(vector_agents[i]))][0]=memory[(int(vector_agents[i]))][0]                                # Memory of cooperation unchanged\n",
    "            memory[(int(vector_agents[i]))][1]=(memory[(int(vector_agents[i]))][1]+Reward[i][0])/2               # Update memory of defection\n",
    "                        \n",
    "    for i in range(M,N):                                                                                 # Loop inside second M agents\n",
    "        if Choice[(int(vector_agents[i]))]==0:                                                               # The agent cooperates\n",
    "            memory[(int(vector_agents[i]))][0]=(memory[(int(vector_agents[i]))][0]+Reward[N-1-i][1])/2           # Update memory of cooperation\n",
    "            memory[(int(vector_agents[i]))][1]=memory[(int(vector_agents[i]))][1]                                # Memory of defection unchanged\n",
    "                        \n",
    "        else:                                                                                                # The agent defects\n",
    "            memory[(int(vector_agents[i]))][0]=memory[(int(vector_agents[i]))][0]                                # Memory of cooperation unchanged\n",
    "            memory[(int(vector_agents[i]))][1]=(memory[(int(vector_agents[i]))][1]+Reward[N-1-i][1])/2           # Update memory of defection\n",
    "                        \n",
    "                  \n",
    "    return(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Perform simulations for different values of parameters (p, k, A), and save results in a .xlsx file #\n",
    "######################################################################################################\n",
    "\n",
    "from datetime import datetime     # Import libraries\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "\n",
    "P = np.arange(0.05, 1, 0.05)      # Define the domain of parameter p \n",
    "K = np.arange(0.05, 1, 0.05)      # Define the domain of parameter k\n",
    "Assort = np.arange(0, 1.01, 0.1)  # Define the domain of parameter A\n",
    "\n",
    "\n",
    "#workbook = openpyxl.load_workbook('name_of_the_file.xlsx')                                 # Open the .xlsx file                    \n",
    "for a in Assort:                                                                            # Loop inside the domain of A\n",
    "    globals()[\"S1_\" + str(int(a*10))] = np.zeros([19,19])\n",
    "    globals()[\"tot_\" + str(int(a*10))] = np.zeros([19,19])\n",
    "    \n",
    "    #worksheet = workbook.create_sheet(r'sheet_%s' %round(a,2))                                 # Create a new sheet in the file for each value of A\n",
    "    \n",
    "    for p in P:                                                                                 # Loop inside the domain of p\n",
    "        for k in K:                                                                                 # Loop inside the domain of k\n",
    "            Simulation=simulation(100,round(p,2),round(k,2),round(a,2),25)                              # Perform simulation;   \n",
    "                                                                                                        # The first argument is the number of time steps T; T=5000 in the simulations of the paper;\n",
    "                                                                                                        # The last argument is the number of couples M;\n",
    "                                                                                                        # The other arguments are the values of p, k, and a that follow from the loops\n",
    "            \n",
    "            now = datetime.now()                                                                         \n",
    "            print(round(a,2),round(p,2),round(k,2),now.strftime(\"%H:%M:%S\"))                            # Print the value of the parameters and the current time after each step of the loops\n",
    "            \n",
    "            globals()[\"S1_\" + str(int(round(a,2)*10))][int(round(p,2)*20-1),int(round(k,2)*20-1)] = Simulation[0]      # For each value of A, generate a matrix with values of p on rows and values of k on columns\n",
    "                                                                                                                       # Example: when A=0.8, call the matrix S1_8; \n",
    "            globals()[\"tot_\" + str(int(round(a,2)*10))][int(round(p,2)*20-1),int(round(k,2)*20-1)] = Simulation[1]\n",
    "            \n",
    "            #worksheet.cell(row=int(round(p,2)*20), column=int(round(k,2)*20)).value = Simulation[0]\n",
    "            #worksheet.cell(row=int(round(p,2)*20+21), column=int(round(k,2)*20)).value = Simulation[1]\n",
    "    \n",
    "    #workbook.save('name_of_the_file.xlsx')    # Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## \n",
    "# Graphic representation #\n",
    "##########################\n",
    "\n",
    "# Import libraries\n",
    "%matplotlib notebook                  \n",
    "import matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "\n",
    "cmap = cm.seismic\n",
    "norm = matplotlib.colors.Normalize(vmin=-1, vmax=1)  # Scale of the color bar\n",
    "\n",
    "X = np.arange(0.05, 1, 0.05)          # Use the same domain of k, K\n",
    "Y = np.arange(0.05, 1, 0.05)          # Use the same domain of p, P\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.pcolormesh(X, Y, S1_10,cmap=cmap,shading='gouraud',norm=norm)     # Change the third argument to plot different results;\n",
    "                                                                    \n",
    "ax.set_xlabel('$K$')\n",
    "ax.set_ylabel('$p$')\n",
    "\n",
    "cax = fig.add_axes([0.905, 0.11, 0.01, 0.77])                        # Position of the colorbar\n",
    "fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),cax=cax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
